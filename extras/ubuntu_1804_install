-----------------------------------------------------------------------
|  UBUNTU 18.04 - AIRFLOW INSTALLATION                                |
-----------------------------------------------------------------------

- First, install the necessary pip packages.

    $ pip install apache-airflow
    $ pip install postgres



- Add an environment variable for the airflow home.

    $ vim ~/.bashrc

    # Add
    export AIRFLOW_HOME="$HOME/airflow"

    $ source ~/.bashrc

Postgres and the LocalExecutor.



- Initialize airflow, which will create a ~/airflow directory.  It will include:
    - a config file 'airflow.cfg'
    - a config file for the webserver 'webserver_config.py'
    - a default SQLite DB 'airflow.db'
    - a folder for logs

    $ airflow db init



- Instead of using SQLite, we'll use Postgres.

    # Create a new Postgres database
    postgres=# CREATE DATABASE airflow;
    postgres=# CREATE USER airflow WITH ENCRYPTED PASSWORD 'airflow';
    postgres=# GRANT ALL PRIVILEGES ON DATABASE airflow to airflow;



- Next, we'll configure airflow to use Postgres and the LocalExecutor.

    # airflow.cfg
    ------------------------------------------------
    [core]
    # Point dags folder to local code repo
    dags_folder = /home/carl/Code/python/airflow/dags
    
    # The executor class that airflow should use. Choices include
    # SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
    executor = LocalExecutor

    # These settings may also need to be changed out of the box
    load_examples = False                # Airflow samples
    base_url = http://localhost:8085     # Web address for UI
    default_timezone = America/Denver    # Specifies local timezone (or can leave default utc)


    [database]
    # Back-end storage url
    sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@localhost:5432/airflow



- Now, we just need to re-run the 'initdb' script to set up Postgres.  We'll also need to add psycopg2.

    $ pip install psycopg2

    $ airflow db init

  Also, when new DAGs are added to the '~/airflow/dags' we will have to run this command 
    again to recognize the new DAG.



- Now, airflow should be completely configured.  We can get it up and running:

    $ airflow scheduler
    $ airflow webserver



- In order to log into the webserver, we need to create a user:

    $ airflow users  create --role Admin --username admin --email admin --firstname admin 
        --lastname admin --password admin



- One last thing is that we have to make some changes if we want airflow to be run 
    automatically at startup by systemd.

  The default systemd service files initially look like this:

    [Unit]
    Description=Airflow scheduler daemon
    After=network.target postgresql.service mysql.service redis.service rabbitmq-server.service
    Wants=postgresql.service mysql.service redis.service rabbitmq-server.service
    
    [Service]
    EnvironmentFile=/etc/sysconfig/airflow
    User=airflow
    Group=airflow
    Type=simple
    ExecStart=/bin/airflow scheduler
    Restart=always
    RestartSec=5s
    
    [Install]
    WantedBy=multi-user.target


  However, this won't work, since the 'EnvironmentFile' protocol doesn't work on Ubuntu 18.
    Instead, we'll comment out that line and add this line instead:

    Environment="PATH=/home/ubuntu/python/envs/airflow/bin:/usr/local/sbin:/usr/local/bin:
    /usr/sbin:/usr/bin:/sbin:/bin"



- We will also want to create systemd service files for the Airflow Scheduler and the
    WebServer if we want them to launch automatically as well.


    # /etc/systemd/system/airflow-scheduler.service
    ----------------------------------------------------
    [Unit]
    Description=Airflow scheduler daemon
    After=network.target postgresql.service mysql.service redis.service rabbitmq-server.service
    Wants=postgresql.service mysql.service redis.service rabbitmq-server.service
    
    [Service]
    #EnvironmentFile=/etc/default/airflow
    Environment="PATH=/home/ubuntu/python/envs/airflow/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
    User=airflow
    Group=airflow
    Type=simple
    ExecStart=/home/ubuntu/python/envs/airflow/bin/airflow scheduler
    Restart=always
    RestartSec=5s
    
    [Install]
    WantedBy=multi-user.target


    # /etc/systemd/system/airflow-webserver.service
    ----------------------------------------------------
    [Unit]
    Description=Airflow webserver daemon
    After=network.target postgresql.service mysql.service redis.service rabbitmq-server.service
    Wants=postgresql.service mysql.service redis.service rabbitmq-server.service
    
    [Service]
    #EnvironmentFile=/etc/default/airflow
    Environment="PATH=/home/ubuntu/python/envs/airflow/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/    bin:/sbin:/bin"
    User=airflow
    Group=airflow
    Type=simple
    ExecStart=/home/ubuntu/python/envs/airflow/bin/airflow webserver -p 8085 --pid /home/ubuntu/airflow/    airflow-webserver.pid
    Restart=on-failure
    RestartSec=5s
    PrivateTmp=true
    
    [Install]
    WantedBy=multi-user.target



- Finally, we can enable them to start.

    $ sudo systemctl enable airflow-scheduler
    $ sudo systemctl start airflow-scheduler
    $ sudo systemctl enable airflow-webserver
    $ sudo systemctl start airflow-webserver