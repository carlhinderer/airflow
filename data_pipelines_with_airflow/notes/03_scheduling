-----------------------------------------------------------------------
|  CHAPTER 3 - SCHEDULING AN AIRFLOW                                  |
-----------------------------------------------------------------------

- Example - Processing User Events

    - Imagine we have a service that tracks user behavior on our website and allows us to
        analyze which pages users (identified by IP address) accessed on our site.

      For marketing purposes, we would like to know how many different pages are accessed 
        by our users and how much time they spend during each visit.  To get an idea of
        how this behavior changes over time, we want to calculate these statistics on a 
        daily basis.  This will allow us to compare changes across different days and time
        periods.


    - The external tracking service does not store data for more than 30 days.  This means we
        need to store and accumulate this data ourselves.  Normally, we'd store this data in
        the cloud, but for this exercise we'll store it locally.


    - Here is the API that allows us to retrieve the full list of available events in the 
        last 30 days:

        curl -o /tmp/events.json http://localhost:5000/events

      It returns a JSON-encoded list of user events we can analyze to calculate our user
        statistics.



- Calculating the Statistics

    - We will break our workflow down into 2 separate tasks:

        1. Fetch the user events
        2. Calculate the statistics


    - To calculate the statistics, we'll load the data into a pandas DataFrame and calculate
        the number of events using a groupby and an aggregation.

        def calculate_stats(input_path, output_path):
            """Calculates event statistics."""
            events = pd.read_json(input_path)
            stats = events.groupby(["date", "user"]).size().reset_index()
            stats.to_csv(output_path, index=False)


    - Then, we end up with this DAG:

        dag = DAG(
           dag_id="user_events",
           start_date=datetime(2015, 6, 1),
           schedule_interval=None,
        )
         
        fetch_events = BashOperator(
           task_id="fetch_events",
           bash_command="curl -o data/events.json https://localhost:5000/events",
           dag=dag,
        )
 
        calculate_stats = PythonOperator(
           task_id="calculate_stats",
           python_callable=_calculate_stats,
           op_kwargs={
               "input_path": "data/events.json",
               "output_path": "data/stats.csv",
           },
           dag=dag,
        )
         
        fetch_events >> calculate_stats



- Defining Scheduling Intervals

    - It makes sense for us to calculate our statistics daily, so we can use the convenient
        '@daily' macro.  This will run our DAG every day at midnight.

        dag = DAG(
            dag_id="user_events",
            schedule_interval="@daily",
            ...
        )


    - We also need to add a start date so that Airflow knows when to start scheduling our DAG
        runs.  

      Note Airflow starts an interval at the end of the interval.  So, if a daily run starts on
        1/1/2020, then the first run will be at midnight the next day.

        import datetime as dt
 
        dag = DAG(
            dag_id="user_events",
            schedule_interval="@daily",
            start_date=dt.datetime(year=2019, month=1, day=1)
        )


    - If we don't specify an end date, the DAG will keep running forever.  If we know there is a
        defined end date, we can specify it.

        dag = DAG(
            dag_id="user_events",
            schedule_interval="@daily",
            start_date=dt.datetime(year=2019, month=1, day=1),
            end_date=dt.datetime(year=2019, month=1, day=5),
        )



- Cron-Based Intervals

    - For more complicated intervals, Airflow allows us to use scheduling intervals using
        the same syntax as cron.

        # ┌─────── minute (0 - 59)
        # │ ┌────── hour (0 - 23)
        # │ │ ┌───── day of the month (1 - 31)
        # │ │ │ ┌───── month (1 - 12)
        # │ │ │ │ ┌──── day of the week (0 - 6) (Sunday to Saturday;
        # │ │ │ │ │      7 is also Sunday on some systems)
        # * * * * *


    - For instance, here are some cron examples:

        0 * * * * = hourly (running on the hour)
        0 0 * * * = daily (running at midnight)
        0 0 * * 0 = weekly (running at midnight on Sunday)

        0 0 1 * * = midnight on the first of every month
        45 23 * * SAT = 23:45 every Saturday


    - We can also use ',' to define collections and '-' to define ranges.

        0 0 * * MON,WED,FRI = run every Monday, Wednesday, Friday at midnight
        0 0 * * MON-FRI = run every weekday at midnight
        0 0,12 * * * = run every day at 00:00 AM and 12:00 P.M.


    - Airflow also has support for serveral macros that represent commonly used scheduling
        intervals:

        Preset          Meaning
        -----------------------------------------------------------------
        @once           Schedule once and only once
        
        @hourly         Run once an hour at the beginning of the hour
        
        @daily          Run once a day at midnight
        
        @weekly         Run once a week at midnight on Sunday morning
        
        @monthly        Run once a month at midnight on the first day of the month
        
        @yearly         Run once a year at midnight on January 1



- Frequency-Based Intervals

    - One limitation of cron expressions is that they are unable to represent certain
        frequency-based schedules.  For instance, you can't define a cron expression that
        will run once every 3 days.

      This is because cron is built to continuously match against the current time to decide
        whether a job should be run.  The advantage of this is that it makes cron stateless,
        but you do lose some expressiveness.


    - To support this type of interval, Airflow also allows you to define scheduling intervals
        in terms of a relative time interval.

        from datetime import timedelta
 
        dag = DAG(
            dag_id="user_events",
            schedule_interval=timedelta(days=3),
            start_date=dt.datetime(year=2019, month=1, day=1),
        )



- Fetching Events Incrementally

    - So far, we are downloading and calculating statistics for the entire catalog of user
        events each day.  Also, we are only downloading events for the past 30 days, which
        means we have no history for further in the past.


    - One way to solve these issues is to change our DAG to load data in an incremental
        fashion.  This is much more efficient, as we are transferring and processing much
        less data.

        # Start date is inclusive, end date is exclusive (This gets 2019-01-01)
        curl -O http://localhost:5000/events?start_date=2019-01-01&end_date=2019-01-02


    - We can implement this in our DAG by changing our bash command:

        fetch_events = BashOperator(
            task_id="fetch_events",
            bash_command="curl -o data/events.json http://localhost:5000/
                events?start_date=2019-01-01&end_date=2019-01-02",
            dag=dag,
        )



- Dynamic Time References using Execution Dates

    - Airflow provides tasks with extra parameters that can be used to determine on which
        interval a task is being executed.

      The 'execution_date' is a timestamp that reflects the start interval for which the DAG
        is being executed.  The end time of the schedule interval is indicated by the 
        'next_execution_date' parameter.  Together, they define the length of a task's
        schedule interval.


    - For example, we can use Airflow's Jinja templating syntax to include the execution
        dates dynamically in our bash command.

        fetch_events = BashOperator(
            task_id="fetch_events",
            bash_command=(
                 "curl -o data/events.json "
                 "http://localhost:5000/events?"
                 "start_date={{execution_date.strftime('%Y-%m-%d')}}"
                 "&end_date={{next_execution_date.strftime('%Y-%m-%d')}}"
            ),
            dag=dag,
        )


    - Since the execution_date parameters are used so often, Airflow also provides shorthand
        for common date formats.

        ds, next_ds, prev_ds                            # YYYY-MM-DD
        ds_nodash, next_ds_nodash, prev_ds_nodash       # YYYYMMDD


    - Using these shorthands, we can rewrite our incremental fetch command:

        fetch_events = BashOperator(
           task_id="fetch_events",
           bash_command="curl -o data/events.json http://localhost:5000/
                         events?start_date={{ds}}&end_date={{next_ds}}",
           dag=dag,
        )



- Partitioning Your Data

    - Right now, each time we fetch events for a day, we are just overwriting the previous
        day's data.  

      We could just append events to the existing JSON file, but we'd have to load the entire
        file when we wanted to do calculations.  Instead, we'll write each day's events to a
        new file.

        def _calculate_stats(**context):
            """Calculates event statistics."""
            input_path = context["templates_dict"]["input_path"]
            output_path = context["templates_dict"]["output_path"]
         
            events = pd.read_json(input_path)
            stats = events.groupby(["date", "user"]).size().reset_index()
            stats.to_csv(output_path, index=False)
         
         
        calculate_stats = PythonOperator(
            task_id="calculate_stats",
            python_callable=_calculate_stats,
            templates_dict={
                "input_path": "data/events/{{ds}}.json",
                "output_path": "data/stats/{{ds}}.csv",
            },
            provide_context=True,
            dag=dag,
        )



- Executing Work in Fixed-Length Intervals

    - As we've seen, we can control when Airflow runs a DAG with 3 parameters:

        1. Start date
        2. Schedule interval
        3. End date (optional)


    - In this interval-based representation of time, a DAG is executed for a given interval
        as soon as the time slot of that interval has passed.

      For example, if we have a DAG that is to run daily and is scheduled to start on 
        2019-01-03, the DAG will run soon after midnight on 2019-01-04.

      However, if we look at the 'execution_date' variable when our tasks are executed, 
        we will actually see an execution date of '2019-01-03'.  This is because the
        'execution_date' is defined as the start of the corresponding interval, rather than
        when our DAG is actually executed.



- Using Backfilling to Fill in Past Gaps

    - As Airflow allows us to define schedule intervals starting from an arbitrary start date,
        we can also define past intervals starting from a start date in the past.

      We can use this property to perform historical runs of our DAG for loading or analyzing
        past datasets.  This is called 'backfilling'.


    - By default, Airflow will schedule and run any past schedule intervals that have not yet
        been run.  

      So, specifying a past start date and activating the corresponding DAG will result in all
        intervals that have passed before the current time being executed.  This behavior
        can be disabled using the 'catchup' parameter.

        dag = DAG(
           dag_id="user_events",
           schedule_interval=timedelta(days=3),
           start_date=dt.datetime(year=2019, month=1, day=1),
           catchup=False,
        )


    - This value can also be controlled from the Airflow config file.


    - Backfilling can also be used to re-process data after we have made changes in our 
        code. 

      For example, say we make a change to our _calc_statistics function to add a new 
        statistic. Using backfilling, we can clear past runs of our calc_statistics task 
        to re-analyze our historical data using the new code.



- Best Practice - Atomicity

    - 