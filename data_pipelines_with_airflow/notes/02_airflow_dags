-----------------------------------------------------------------------
|  CHAPTER 2 - ANATOMY OF AN AIRFLOW DAG                              |
-----------------------------------------------------------------------

- Rocket Example

    - For our first simple example, we are a rocket enthusiast and want to collect
        pictures of rockets that will be launched soon.

      We'll make a REST call to an API that returns a list of upcoming launches, and
        then we'll take the image URL from the JSON of each launch returned.

      Then, we'll use Python code to retrieve and save the image from each URL.


    - Our workflow looks like:

        download_launches  -->  get_pictures  -->  notify



- DAG of Tasks

    dag = DAG(
       dag_id="download_rocket_launches",
       start_date=airflow.utils.dates.days_ago(14),
       schedule_interval=None,
    )
     
    download_launches = BashOperator(
       task_id="download_launches",
       bash_command="curl -o /tmp/launches.json 'https://launchlibrary.net/1.4/launch?next=5&mode=verbose'",
       dag=dag,
    )
    
    # The get_pictures() method is defined in the script
    get_pictures = PythonOperator(
       task_id="get_pictures",
       python_callable=_get_pictures,
       dag=dag,
    )
     
    notify = BashOperator(
       task_id="notify",
       bash_command='echo "There are now $(ls /tmp/images/ | wc -l) images."',
       dag=dag,
    )
     
    download_launches >> get_pictures >> notify



- Tasks vs Operators

    - 'Task' and 'operator' are often used interchangeably, but they have a subtle difference.  


    - Operators provide the implementation of a piece of work.  Tasks manage the execution of an
        operator.  They can be thought of as a small wrapper that ensures the operator executes
        correctly.



- Running Airflow in a Python Environment

    - The bare minimum Airflow consists of 3 core components:

        1. Scheduler
        2. Webserver
        3. Database


    - To install Airflow locally:

        $ pip install apache-airflow


    - We initialized the metastore, a database where all Airflow state is stored.  For instructions on
        setting up Postgres as the DB, check 'ubuntu_1804_install'.

        $ airflow db init


    - Then, we create a user:

        $ airflow users create --username admin --password admin --firstname Anonymous \
            --lastname Admin --role Admin --email admin@example.org


    - We copy the DAG we just created into the DAGs folder:

        $ cp download_rocket_launches.py ~/airflow/dags


    - Now, we can start the webserver and scheduler:

        $ airflow webserver &
        $ airflow scheduler &


    - Navigate to the web UI at http://localhost:8080.  We can see our task, and can click 'Trigger DAG' 
        to run it.


    - To look at the outcome of our DAG after it has finished, we can click on the 'notify'
        action in the 'Graph View' and click 'View Logs'.



- Running Airflow in Docker Containers

    - To run Airflow in a Docker container:

        $ docker run \
            -it \
            -p 8080:8080 \
            -v /path/to/dag/download_rocket_launches.py:/opt/airflow/dags/download_rocket_launches.py \
            --entrypoint=/bin/bash \
            --name airflow \
            apache/airflow:2.0.0-python3.8 \
            -c '( \
              airflow db init && \
              airflow users create --username admin --password admin --firstname
              Anonymous --lastname Admin --role Admin --email admin@example.org \
              ); \
              airflow webserver & \
              airflow scheduler \
              '


    - Note that this is just a quickstart.  In Production, you should run the webserver, scheduler, and
        metastore in separate containers.



- Running at Regular Intervals

    - We can schedule a DAG to run at certain intervals by setting the 'schedule_interval'
        argument.

        dag = DAG(
           dag_id="download_rocket_launches",
           start_date=airflow.utils.dates.days_ago(14),
           schedule_interval="@daily",
        )



- Handling Failing Tasks

    - If a task fails, it will throw some kind of error.  It will be shown in red in the graph.  We can 
        look at the logs to see why the task failed.


    - It is unnecessary to re-run the entire DAG.  We can click the failed task and select 'Clear',
        which will reset the state of the task.  This will cause Airflow to re-run the failed task
        and any subsequent tasks.